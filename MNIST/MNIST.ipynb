{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification\n",
    "\n",
    "In this notebook, we'll explore the MNIST (Modified National Institute of Standards and Technology) database - a large set of handwritten digits with their associated labels.  A common Machine Learning challenge is to build a multiclass classifier than can tell with high accuracy what digit a given image represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules for maths, data manipulation and plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch and load data\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "data, labels = mnist[\"data\"], mnist[\"target\"].astype(np.uint8)\n",
    "\n",
    "# Select three arbitrary digits\n",
    "example_digits = data[40:43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up plotting area\n",
    "fig = plt.figure(figsize=(12,3))\n",
    "\n",
    "# Set up subplots for each digit - we'll plot each one side by side to illustrate the variation\n",
    "ax1, ax2, ax3 = fig.add_subplot(131), fig.add_subplot(132), fig.add_subplot(133)\n",
    "axs = [ax1, ax2, ax3]\n",
    "\n",
    "# Plot the digits\n",
    "for i in range(3):\n",
    "    \n",
    "    ax = axs[i]\n",
    "    ax.imshow(example_digits[i].reshape(28, 28), cmap=\"binary\")\n",
    "    ax.set_xticks([], []) \n",
    "    ax.set_yticks([], []) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going any further, let's split the data into training and test sets.  Then we can explore the training set all we like without risking becoming biased and giving ourselves an unfair advantage when we eventually come to evaluate the performance of our model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, labels_train, labels_test = data[:60000], data[60000:], labels[:60000], labels[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplify the problem - Binary Classifier\n",
    "\n",
    "As a first step, let's simplify the classification problem and build a binary classifier.  Let's see if we can train a model to recognise 7's.  The two possible outcomes for this scenario are then simply *7* and *not 7*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train_7 = (labels_train == 7) # True for 7's, false for all other digits\n",
    "labels_test_7 = (labels_test == 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's select and train a classifier.  A good place to start is with a *Stochastic Gradient Descent* (SGD) classifier as they can handle large datasets efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(random_state=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf_7 = SGDClassifier(random_state=0) # Instantiate model\n",
    "sgd_clf_7.fit(data_train, labels_train_7) # Train to recognise 7's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on our three example digits from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf_7.predict(example_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How delightful - our classifier has got it right.  Now, let's see how it does when we cross-validate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9754 , 0.9795 , 0.98115])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(sgd_clf_7, data_train, labels_train_7, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too shabby!  Our classifer scores at least 96% accuracy in distinguishing sevens from not-sevens.  But wait - is this really so impressive?  Let's construct the stupidest model we can - one that always guesses that the digit isn't a seven.  There are a lot of not-sevens in the dataset, so it might end up doing quite well despite its lack of intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89535, 0.8984 , 0.893  ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class AlwaysNot7(BaseEstimator):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self # Fitting does nothing\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool) # Return an array made up of one False per instance of data\n",
    "\n",
    "always_not_7_clf = AlwaysNot7()\n",
    "\n",
    "cross_val_score(always_not_7_clf, data_train, labels_train_7, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dumb model scores 89% accuracy!  We're still significantly more accurate when we actually train our model, but this exercise just shows that model performance is relative and needs to be compared to a suitable baseline to properly evaluate it.\n",
    "\n",
    "Now, instead of just coming up with the cross validation accuracy scores, let's actually get a set of predictions.  We can use cross_val_predict to get a \"clean\" set of predictions on the training data - ie predictions for each instance in the training data are obtained by training the model on a set of folds *which do not contain the instance being predicted.*  In this way, we ensure that the model can't cheat by seeing the answer before making a prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-bc19386382ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata_train_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msgd_clf_7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train_7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataquest\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataquest\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_predict\u001b[1;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, pre_dispatch, method)\u001b[0m\n\u001b[0;32m    771\u001b[0m     prediction_blocks = parallel(delayed(_fit_and_predict)(\n\u001b[0;32m    772\u001b[0m         clone(estimator), X, y, train, test, verbose, fit_params, method)\n\u001b[1;32m--> 773\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m     \u001b[1;31m# Concatenate the predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataquest\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1002\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataquest\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    833\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataquest\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataquest\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataquest\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataquest\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataquest\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataquest\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_predict\u001b[1;34m(estimator, X, y, train, test, verbose, fit_params, method)\u001b[0m\n\u001b[0;32m    860\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataquest\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m    725\u001b[0m                          \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m                          \u001b[0mcoef_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mintercept_init\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m                          sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataquest\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         self._partial_fit(X, y, alpha, C, loss, learning_rate, self.max_iter,\n\u001b[1;32m--> 566\u001b[1;33m                           classes, sample_weight, coef_init, intercept_init)\n\u001b[0m\u001b[0;32m    567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         if (self.tol is not None and self.tol > -np.inf\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataquest\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, classes, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[0;32m    523\u001b[0m                              \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m                              \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m                              max_iter=max_iter)\n\u001b[0m\u001b[0;32m    526\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             raise ValueError(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataquest\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit_binary\u001b[1;34m(self, X, y, alpha, C, sample_weight, learning_rate, max_iter)\u001b[0m\n\u001b[0;32m    582\u001b[0m                                               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_expanded_class_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m                                               \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m                                               random_state=self.random_state)\n\u001b[0m\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataquest\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\u001b[0m in \u001b[0;36mfit_binary\u001b[1;34m(est, i, X, y, alpha, C, learning_rate, max_iter, pos_weight, neg_weight, sample_weight, validation_mask, random_state)\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 438\u001b[1;33m         est.eta0, est.power_t, est.t_, intercept_decay, est.average)\n\u001b[0m\u001b[0;32m    439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "data_train_predictions = cross_val_predict(sgd_clf_7, data_train, labels_train_7, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mx_7 = confusion_matrix(labels_train_7, data_train_predictions)\n",
    "conf_mx_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the following results:\n",
    "- 53,321 True Positives (model predicted 7, was actually a 7);\n",
    "- 414 False Negatives (model predicted not-7, was actually a 7);\n",
    "- 865 False Positives (model predicted 7, was actually not a 7); and\n",
    "- 5400 True Negatives (model predicted not-7, was actually not a 7).\n",
    "\n",
    "## Precision and Recall\n",
    "\n",
    "*Precision* is the ratio of True Positives to positive predictions, ie TP / (TP + FP).  Precision represents how reliable the model's positive predictions are.  \n",
    "\n",
    "*Recall* is the ratio of True Positives to all positive instances in the dataset, ie TP / (TP + FN).  Recall represents how good the model is at catching positive instances.  \n",
    "\n",
    "The mathematics of calculating precision and recall is not particularly demanding for this example, but nonetheless, they can easily be calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(precision_score(labels_train_7, data_train_predictions))\n",
    "print(recall_score(labels_train_7, data_train_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model thus has around 93% precision and 86% recall.  The precision and recall are often combined into a single metric for convenience—the F1 score.  It's calculated as the harmonic mean of the precision and recall (this means that we will only get a high F1 score if both the precision and recall are high.  If either one is low, then it will drag the average down significantly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f1_score(labels_train_7, data_train_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As both our precision and recall are pretty high, the F1 score is also quite good.  On the whole, our model isn't half bad at identifying sevens.  Perhaps it would struggle with more ambiguous numbers, though—the only digit that is vagualy similar to 7 is 1.  \n",
    "\n",
    "Our SGD classifier makes decisions by calculating a *decision score* for each instance.  If the decision score for a given instance is above a particular value, then that instance is predicted to be a positive result—otherwise it is classified as negative.  If we were able to increase the threshold, then the model would give more negative predictions and would only give predictions for the instances that it was more sure about (which would have higher decision scores).\n",
    "\n",
    "We cannot set the threshold of our SGD classifier directly, but we can access the decision scores that the model produces for a given set of data.  We can then set our own threshold, compare it with the decision scores, and infer what the model would have throught if it had used that threshold.  We can get decision scores using cross_val_predict with the method parameter set to \"decision_function\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_decision_scores = cross_val_predict(sgd_clf_7, data_train, labels_train_7, cv=3,\n",
    "                                                method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "\n",
    "print(sum(data_train_decision_scores > threshold))\n",
    "\n",
    "threshold = 1000 # Require model to be more confident to classify as positive by raising decision score threshold\n",
    "\n",
    "print(sum(data_train_decision_scores > threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raising the threshold reduces the number of positive predictions.  However, this will typically mean that there are now more false negatives—positive instances that the model would have correctly identified if it was allowed to be less prudent with its estimates.  \n",
    "\n",
    "We can plot how the precision and recall vary with the threshold.  By doing this, we can select a threshold that gives us an acceptable balance of precision and recall.  Whether we require high precision, high recall, or indeed both, depends on the particular situation in which the model will be applied.  In some circumstances, high recall may be preferred over high precision if there is little cost associated with a few false alarms (false positives) but high costs associated with missing a positive instance (false negatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_decision_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(labels_train_7, data_train_decision_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plotting area\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "# Plot decision and recall curves\n",
    "ax.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "ax.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "\n",
    "# Add legend to distinguish lines\n",
    "ax.legend(loc=\"center left\", fontsize=12)\n",
    "\n",
    "# Set x axis label\n",
    "ax.set_xlabel(\"Threshold\", labelpad=20, fontsize=12)\n",
    "\n",
    "# Set appropriate x- and y-axis limits\n",
    "ax.axis([-50000,50000,0,1.01])\n",
    "\n",
    "\n",
    "# What threshold should we set if we wanted 95% precision?  Use np.argmax()\n",
    "# (precisions >= 0.95) is an array of [False, False, ..., False, True, True, ...]\n",
    "# With the first True representing the lowest threshold for which precision in greater than 95%\n",
    "# np.argmax() sees True as 1 and False as 0 so it takes the first instance of the maximum value in the array and\n",
    "# returns the index position in the array.  We can use this index to find the corresponding threshold and recall.\n",
    "threshold_95_precision = thresholds[np.argmax(precisions >= 0.95)]\n",
    "recall_95_precision = recalls[np.argmax(precisions >= 0.95)]\n",
    "\n",
    "# Plot this threshold and the associated precision and recall\n",
    "ax.plot([threshold_95_precision, threshold_95_precision], [0.0, 0.95], \"r:\")\n",
    "ax.plot([-50000, threshold_95_precision], [0.95, 0.95], \"r:\")\n",
    "ax.plot([-50000, threshold_95_precision], [recall_95_precision, recall_95_precision], \"r:\")\n",
    "\n",
    "ax.plot(threshold_95_precision, recall_95_precision, \"ro\")\n",
    "ax.plot(threshold_95_precision, 0.95, \"ro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark that as the decision score threshold is increased, the number of true positives can only decrease.  Similarly, the number of false positives can only decrease.  However, the number of false negatives will increase.  \n",
    "\n",
    "Because both true and false positives can decrease, it is possible for precision to decrease on a local scale as the threshold is increased - although the trend will generallly be positive.  This is why the pecision curve is bumpy.  Conversely, recall can only decrease as the threshold is increased, leading to a smooth curve.\n",
    "\n",
    "The reason for the dip in precision at high thresholds is as follows: when the threshold is already high, the model will only predict positive for instances that it is very confident about, and there will be very few false positives—if the model has even a small doubt about a particular instance, it will class if as negative.  So the effect of increasing the threshold even higher begins to reduce true positives while having little effect on false positives.  This causes recall to fall rather continuously.\n",
    "\n",
    "Another useful way of visualising the tradeoff between precision and recall is to simply plot them against eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plotting area\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "# Plot curve\n",
    "ax.plot(recalls, precisions, \"b-\")\n",
    "\n",
    "# Set x-axis label\n",
    "ax.set_xlabel(\"Recall\", labelpad=20, fontsize=12)\n",
    "\n",
    "# Set y-axis label\n",
    "ax.set_ylabel(\"Precision\", labelpad=20, fontsize=12)\n",
    "\n",
    "# Set appropriate x- and y-axis limits\n",
    "ax.axis([0,1,0,1])\n",
    "\n",
    "# Plot recall corresponding to 95% precision\n",
    "ax.plot([recall_95_precision, recall_95_precision], [0, 0.95], \"r:\")\n",
    "ax.plot([0, recall_95_precision], [0.95, 0.95], \"r:\")\n",
    "\n",
    "ax.plot(recall_95_precision, 0.95, \"ro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that for this particular classifer, we can achieve a high precision without sacrificing much recall.  However, precision really starts to fall off sharply at around 90% recall - so we wouldn't want to try to increase recall further than that.  The same shape is true in general for precision/recall of classifiers - there will be a point where increasing recall begins to have a sharply noticeable detrimental effect on precision.  We would usually want to balance the two quantities at a point just before this drop off in the curve.\n",
    "\n",
    "## ROC (Receiver Operating Characteristic) Curve\n",
    "\n",
    "The ROC curve is another common tool for evaluating the performance of binary classifiers.  It takes the form of a plot of the true positive rate (TPR) against the false positive rate (FPR) as the decision score threshold is varied (for a given threshold, there is only one TPR and FPR).  We can use the roc_curve function from sklearn to easily compute the TPR and FPR for various thresholds.\n",
    "\n",
    "For comparison, let's also train a Random Forest model and see how its ROC curve compares to that of our SGD model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf_7 = RandomForestClassifier(random_state=0)\n",
    "data_train_rf_probabilities = cross_val_predict(rf_clf_7, data_train, labels_train_7, cv=3,\n",
    "                                               method=\"predict_proba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_rf_decision_scores = data_train_rf_probabilities[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr_sgd, tpr_sgd, thresholds_sgd = roc_curve(labels_train_7, data_train_decision_scores)\n",
    "fpr_rf, tpr_rf, thresholds_tree = roc_curve(labels_train_7, data_train_rf_decision_scores)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    ax.plot(fpr, tpr, label=label)\n",
    "    ax.plot([0,1], [0,1], \"k--\") # Dashed diagonal line - random model\n",
    "    \n",
    "    ax.axis([-0.001,1.001,-0.001,1.001])\n",
    "    ax.legend(loc=\"center right\")\n",
    "    ax.set_xlabel(\"False Positive rate\", labelpad=10)\n",
    "    ax.set_ylabel(\"True Positive rate\", labelpad=10)\n",
    "\n",
    "plot_roc_curve(fpr_sgd, tpr_sgd, \"SGD\")\n",
    "plot_roc_curve(fpr_rf, tpr_rf, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve for a superior model is further towards the top left hand corner of the plot—that is, further away from the diagonal line across the middle, which represents a random model (that's just makes a guess rather than any kind of \"prediction\").  We can hence see that the Random Forest is superior to the SGD model in this example.\n",
    "\n",
    "The AUC, or Area Under Curve, is simply that - the area under the ROC curve.  A perfect model will have an AUC of 1, a random model will have an AUc of 0.5, and a the perfect worse-than-random model will have an AUC of 0 (all predictions are precisely the wrong way round).\n",
    "\n",
    "\n",
    "## Multiclass Classification\n",
    "\n",
    "Some classifiers can handle multiple classes natively, such as SGD, Naive Bayes, and Random Forests.  Some other algorithms can only handle binary classification.  However, we can break a multiclass classification down into several smaller binary classification problems.  To do this, we can either use a One-versus-One (OvO) or One-versus-Rest (OvR) approach.\n",
    "\n",
    "In One-versus-One, we as many binary classifiers as there are binary combinations.  For this MNIST dataset, there are 10 possible classes, so we would have one classifier each for 1s vs 2s, 1s vs 3s, ..., 2s vs 3s, ..., and so on.  When it comes to the classification of an instance, the data is run through every classifier and the output class that \"wins\" the most is used to predict the final output.  This results in us having 45 classifiers—which is a lot!  Fortunately, each classifier only needs to be trained on a smaller part of the training set that relates to the two classes that it must be able to distinguish.\n",
    "\n",
    "In One-versus-Rest, we have as many binary classifiers as there are classes.  For out MNIST dataset, this is 10.  We have one classifier that tests 0 vs not-0, one for 1 vs not-1, and so on.  The classifier that is the most confident in its result (ie has the highest decision score) is used to predict the final output.\n",
    "\n",
    "To illustrate, let's take an algorithm that cannot natively deal with multiple classes - a Support Vector Machine (SVM) classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC()\n",
    "svm_clf.fit(data_train, labels_train)\n",
    "svm_clf.predict(example_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, our SVM classifier used the OvO strategy (by default).  If we wanted to force it to use OvR instead, we would have to pass the SVC class to the OneVsRestClassifier constructor as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "ovr_clf = OneVsRestClassifier(SVC())\n",
    "ovr_clf.fit(data_train, labels_train)\n",
    "ovr_clf.predict(example_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Project Workflow\n",
    "\n",
    "Now, let's pretend as if we were in a real-world scenario and needed to develop and train a model from start to finish to solve a business problem.  We have a certain number of steps that we would follow, including:\n",
    "\n",
    "- exploring the data;\n",
    "- preprocessing the data;\n",
    "- exploring different candidate algorithms for the model; and\n",
    "- fine-tuning hyperparameters to optimise performance.\n",
    "\n",
    "These steps are taken from Appendix B of \"Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow\" by Aurélien Géron.  Check it out [here!](https://www.amazon.co.uk/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=pd_lpo_14_t_0/257-3076774-3184456?_encoding=UTF8&pd_rd_i=1492032646&pd_rd_r=65d5ebf9-18f7-4ff1-be37-f64b7c27264a&pd_rd_w=SJsP8&pd_rd_wg=JveV2&pf_rd_p=7b8e3b03-1439-4489-abd4-4a138cf4eca6&pf_rd_r=6QHV19BYZBXWR68C2G07&psc=1&refRID=6QHV19BYZBXWR68C2G07)\n",
    "\n",
    "### 1.  Framing the problem\n",
    "\n",
    "*Define the objective in business terms.*  Perhaps we have a large library of physical documents that we need to transcribe.  Doing it by hand may be painstaking, time-consuming and expensive.  We may want to invest in the creation of a model to help us save time and resources in the long run.\n",
    "\n",
    "*How will the solution be used?*  The solution will be used to replace several temporary staff who are currently required to transcribe the library documents by hand.  A smaller staff will then only need to scan in the documents to be processed by the model.  \n",
    "\n",
    "*Current solutions and workarounds.*  The current solution is to have a physical staff transcribing documents.  Another option would be to hire a small staff to scan in the documents, but then outsource the transcription—either by forwarding on the scans to a specialist transcription company, or to a crowdsourcing marketplace such as Amazon Mechanical Turk.\n",
    "\n",
    "*How should we frame this problem?*  This problem should clearly be framed as a supervised learning task—we do not want digits to be classified into groups as we alreay know the classes.  We have a large bank of labelled trianing data, which is perfect for training a supervised algorithm.  As the model will only be used for this single library of documents, there is not much value in designing an online learning system that can receive a constant stream of new data.  This data is not changing rapidly and there is no real need to update the model beyond what is required for this specific set of documents.  We will use a model-based learning approach, as instance-based learning could be flawed.  Some digits may be written in an ambiguous way which means that they could be misclassified if they are deemed to be \"closer\" to the wrong digit.\n",
    "\n",
    "*How should performance be measured?*  We will use precision and recall to assess model performance.  We want our model to be confident in its predictions, and hence to have a high precision.  However, we also want our model to avoid false negatives, as these could be costly to identify and fix.  We want both high precision and recall, so we can also use the F1 score metric—which takes the harmonic mean of the precision and recall.  To obtain a high F1 score, the model would need to have both high precision and recall.  We could potentially collate difficult digits that our model only has a low degree of confidence with and send them off to a crowdsourcing marketplace for further human input—but we would want to minimise these cases to save on costs.\n",
    "\n",
    "*Is the performance measure aligned with the business objective?*  The consequences of misclassifcation are not experienced immediately, but if a sufficient number of errors are made by the model then it could undermine the credibility of our work.  It could also mean that we need to re-do the process, which would be time-consuming and expensive.  Thus, precision and recall are both relevant metrics to the business objective.\n",
    "\n",
    "*What would the minimum performance need to be to reach the business objective?*  Even though it takes much longer, human classification and transcription is not perfect and is only around 97% accurate.  Our bosses have told us that if we can match or beat this metric, then we can deploy the model as even for the same level of accuracy the model will be much cheaper to run that the human operation.\n",
    "\n",
    "*What are comparable problems?  Can we reuse experience of tools?*  Similar problems would be text and letter recognition—we essentially have the same problem but with digits.  Other similar problems would be other image recognition tasks, where a model needs to learn to distringuish between a range of particular objects in a given image.  We could do some research to see if there are any pre-buily models or modelling systems that can be easily adapted to our data.\n",
    "\n",
    "*Is human expertise available?*  Yes—most humans over the age of 5 will probably be able to classify digits in the way that we want!  Techincal expertise is not needed to understand the problem and evaluate whether the model has made a correct or incorrect prediction.  We can easily access crowdsourcing platforms if we need human input on any stage of the process.\n",
    "\n",
    "...\n",
    "\n",
    "There are many, many considerations we would need to take if we were applying our model to a real-world scenario.  In the interests of time we will not run through absolutely all of them!  Briefly, other considerations include (but may not be limited to):\n",
    "\n",
    "- How would the problem be solved manually?\n",
    "- What assumptions have we, or others, made so far?\n",
    "- Can we verify these assumptions?\n",
    "\n",
    "### 2.  Get the Data\n",
    "\n",
    "- Automate the data collection process as much as possible so we can easily get fresh data\n",
    "- What data do we need, and how much of it?\n",
    "- Find and document where we can get the data\n",
    "- How much space will the data take up?\n",
    "- Are there any legal obligations and requirements associated with procurement and ownership of the data?\n",
    "- Get access authorisations\n",
    "- Create a workspace with sufficient storage space\n",
    "- Get the data!\n",
    "- Convert the data into a format that is easy to manipulate, without changing the data itself\n",
    "- Ensure sensitive information is deleted or protected—eg anonymised in such a way that the original data cannot be re-engineered\n",
    "- Check size and type of data—time series, sample, geographical, ...?\n",
    "- Sample a test set and set it aside - don't ever look at it to avoid (either consciously or unconsciously) learning something about the test set that may bias your model development process and cause you to overfit to the test set, rather than minimising generalisation error\n",
    "\n",
    "### 3. Explore the Data\n",
    "\n",
    "- Create a copy of the data for exploration.  Sample it down to a more manageable size if necessary\n",
    "- Keep a record of your data exploration—in a Jupyter notebook for example\n",
    "- Study each attribute of the data and its characteristics:\n",
    "    - Name\n",
    "    - Data type\n",
    "    - % missing values\n",
    "    - Noisiness, and type of noise if applicable (random, outliers, rounding errors, etc)\n",
    "    - Usefullness of attribute for the task\n",
    "    - Distribution of attribute (normal, uniform, log normal, etc)\n",
    "- For supervised learning tasks, identify the target attribute or attributes\n",
    "- Visualise the data to get a better understanding of it, and to make these insights easier to communicate\n",
    "- Study correlations between attributes\n",
    "- Study how to solve the problem manually\n",
    "- Identify promising data tranformations that you might want to apply—scaling, normalisation, etc\n",
    "- Identify additional data the would be useful at this stage—go back to the *Get the Data* stage and go through the process of obtaining it\n",
    "- Document what you have learned for the benefit of others (including future-you!) \n",
    "\n",
    "### 4. Prepare the data\n",
    "\n",
    "- Clean data—fix or remove outliers, fill in missing values or drop rows/columns\n",
    "- Select features—drop attributes that aren't useful\n",
    "- Engineer features—discretise continuous values, decompose features, add tranformations, aggregate features\n",
    "- Scale features—standardise or normalise \n",
    "\n",
    "We should work on *copies* of the data when preparing it.  We should also write functions for any data transformations for several reasons:\n",
    "\n",
    "- Easy to prepare a new dataset\n",
    "- Can apply the same transformations in future projects\n",
    "- Can clean and prepare test set in same way as training set\n",
    "- Can clean and prepare new data when the solution is live\n",
    "- Can treat preparating choices (eg certain data transformations are feature additions) as hyperparameters \n",
    "\n",
    "### 5. Shortlist promising models\n",
    "\n",
    "- Train several quick-and-dirty models from various categories\n",
    "- Measure and compare performance between them\n",
    "- Analyse the most significant variables for each algorithm\n",
    "- Analyse the types of errors each model makes\n",
    "- Perform feature selection/engineering to see if these models can be significantly improved\n",
    "- Iterate previous steps a couple of times \n",
    "- Shortlist top few models—ideally choosing models that make different types of errors\n",
    "\n",
    "We may want to sample a smaller training set so we can train many models without taking up too much time.  However, we should be aware that this penalises complex models that require a lot of data the \"get into their stride\", so to speak.  As with the data preparation step, we want to automate as much as possible in this step.\n",
    "\n",
    "### 6. Fine-tune the system\n",
    "\n",
    "- Fine-tune hyperparameters using cross-validation.  If we're unsure about some of the data preparation/transformation steps, we can treat them asa hyperparameters here to see if they actually improve model performance when included/excluded.  Use random search rather than grid search, unless there are very few hyperparameters so that the hyperparameter space can be searched relatively thoroughly.\n",
    "- Try ensemble methods by combining the best models—this is where having models that make different kinds of errors comes in handy as they may be able to cover each other's weaknesses\n",
    "- Once confident in a final model, measure its performance and generalisation error on the test set.  After this, don't tweak the model any further—or we run the risk of overfitting the test set.\n",
    "\n",
    "### 7. Present the solution\n",
    "\n",
    "- Document the modelling process\n",
    "- Create a presentation—highlight the big picture\n",
    "- Explain how the solution addresses the inital business problem\n",
    "- Present interesting points that were picked up along the way\n",
    "- Use eye-pleasing visualisations and simple, easy-to-remember statements to communicate the key findings\n",
    "\n",
    "### 8. Launch the model\n",
    "\n",
    "- Prepare the solution for production\n",
    "- Write code to monitor model performance at regular intervals—ensure that drops in performance will trigger an alert\n",
    "- Retrain the model on fresh data on a regular basis with fresh data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
