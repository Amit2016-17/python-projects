{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification\n",
    "\n",
    "In this notebook, we'll explore the MNIST (Modified National Institute of Standards and Technology) database - a large set of handwritten digits with their associated labels.  A common Machine Learning challenge is to build a multiclass classifier than can tell with high accuracy what digit a given image represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules for maths, data manipulation and plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch and load data\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "data, labels = mnist[\"data\"], mnist[\"target\"].astype(np.uint8)\n",
    "\n",
    "# Select three arbitrary digits\n",
    "example_digits = data[40:43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up plotting area\n",
    "fig = plt.figure(figsize=(12,3))\n",
    "\n",
    "# Set up subplots for each digit - we'll plot each one side by side to illustrate the variation\n",
    "ax1, ax2, ax3 = fig.add_subplot(131), fig.add_subplot(132), fig.add_subplot(133)\n",
    "axs = [ax1, ax2, ax3]\n",
    "\n",
    "# Plot the digits\n",
    "for i in range(3):\n",
    "    \n",
    "    ax = axs[i]\n",
    "    ax.imshow(example_digits[i].reshape(28, 28), cmap=\"binary\")\n",
    "    ax.set_xticks([], []) \n",
    "    ax.set_yticks([], []) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going any further, let's split the data into training and test sets.  Then we can explore the training set all we like without risking becoming biased and giving ourselves an unfair advantage when we eventually come to evaluate the performance of our model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, labels_train, labels_test = data[:60000], data[60000:], labels[:60000], labels[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, let's simplify the classification problem and build a binary classifier.  Let's see if we can train a model to recognise 7's.  The two possible outcomes for this scenario are then simply *7* and *not 7*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train_7 = (labels_train == 7) # True for 7's, false for all other digits\n",
    "labels_test_7 = (labels_test == 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's select and train a classifier.  A good place to start is with a *Stochastic Gradient Descent* (SGD) classifier as they can handle large datasets efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(random_state=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf_7 = SGDClassifier(random_state=0) # Instantiate model\n",
    "sgd_clf_7.fit(data_train, labels_train_7) # Train to recognise 7's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on our three example digits from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf_7.predict(example_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How delightful - our classifier has got it right.  Now, let's see how it does when we cross-validate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9754 , 0.9795 , 0.98115])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(sgd_clf_7, data_train, labels_train_7, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too shabby!  Our classifer scores at least 96% accuracy in distinguishing sevens from not-sevens.  But wait - is this really so impressive?  Let's construct the stupidest model we can - one that always guesses that the digit isn't a seven.  There are a lot of not-sevens in the dataset, so it might end up doing quite well despite its lack of intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89535, 0.8984 , 0.893  ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class AlwaysNot7(BaseEstimator):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self # Fitting does nothing\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool) # Return an array made up of one False per instance of data\n",
    "\n",
    "always_not_7_clf = AlwaysNot7()\n",
    "\n",
    "cross_val_score(always_not_7_clf, data_train, labels_train_7, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dumb model scores 89% accuracy!  We're still significantly more accurate when we actually train our model, but this exercise just shows that model performance is relative and needs to be compared to a suitable baseline to properly evaluate it.\n",
    "\n",
    "Now, instead of just coming up with the cross validation accuracy scores, let's actually get a set of predictions.  We can use cross_val_predict to get a \"clean\" set of predictions on the training data - ie predictions for each instance in the training data are obtained by training the model on a set of folds *which do not contain the instance being predicted.*  In this way, we ensure that the model can't cheat by seeing the answer before making a prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "data_train_predictions = cross_val_predict(sgd_clf_7, data_train, labels_train_7, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[53321,   414],\n",
       "       [  865,  5400]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mx_7 = confusion_matrix(labels_train_7, data_train_predictions)\n",
    "conf_mx_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the following results:\n",
    "- 53,321 True Positives (model predicted 7, was actually a 7);\n",
    "- 414 False Negatives (model predicted not-7, was actually a 7);\n",
    "- 865 False Positives (model predicted 7, was actually not a 7); and\n",
    "- 5400 True Negatives (model predicted not-7, was actually not a 7).\n",
    "\n",
    "*Precision* is the ratio of True Positives to positive predictions, ie TP / (TP + FP).  Precision represents how reliable the model's positive predictions are.  \n",
    "\n",
    "*Recall* is the ratio of True Positives to all positive instances in the dataset, ie TP / (TP + FN).  Recall represents how good the model is at catching positive instances.  \n",
    "\n",
    "The mathematics of calculating precision and recall is not particularly demanding for this example, but nonetheless, they can easily be calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9287925696594427\n",
      "0.8619313647246608\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(precision_score(labels_train_7, data_train_predictions))\n",
    "print(recall_score(labels_train_7, data_train_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model thus has around 93% precision and 86% recall.  The precision and recall are often combined into a single metric for convenience—the F1 score.  It's calculated as the harmonic mean of the precision and recall (this means that we will only get a high F1 score if both the precision and recall are high.  If either one is low, then it will drag the average down significantly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8941137511383392\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f1_score(labels_train_7, data_train_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As both our precision and recall are pretty high, the F1 score is also quite good.  On the whole, our model isn't half bad at identifying sevens.  Perhaps it would struggle with more ambiguous numbers, though—the only digit that is vagualy similar to 7 is 1.  \n",
    "\n",
    "Our SGD classifier makes decisions by calculating a *decision score* for each instance.  If the decision score for a given instance is above a particular value, then that instance is predicted to be a positive result—otherwise it is classified as negative.  If we were able to increase the threshold, then the model would give more negative predictions and would only give predictions for the instances that it was more sure about (which would have higher decision scores).\n",
    "\n",
    "We cannot set the threshold of our SGD classifier directly, but we can access the decision scores that the model produces for a given set of data.  We can then set our own threshold, compare it with the decision scores, and infer what the model would have throught if it had used that threshold.  We can get decision scores using cross_val_predict with the method parameter set to \"decision_function\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_decision_scores = cross_val_predict(sgd_clf_7, data_train, labels_train_7, cv=3,\n",
    "                                                method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5814\n",
      "5542\n"
     ]
    }
   ],
   "source": [
    "threshold = 0\n",
    "\n",
    "print(sum(data_train_decision_scores > threshold))\n",
    "\n",
    "threshold = 1000 # Require model to be more confident to classify as positive by raising decision score threshold\n",
    "\n",
    "print(sum(data_train_decision_scores > threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raising the threshold reduces the number of positive predictions.  However, this will typically mean that there are now more false negatives—positive instances that the model would have correctly identified if it was allowed to be less prudent with its estimates.  \n",
    "\n",
    "We can plot how the precision and recall vary with the threshold.  By doing this, we can select a threshold that gives us an acceptable balance of precision and recall.  Whether we require high precision, high recall, or indeed both, depends on the particular situation in which the model will be applied.  In some circumstances, high recall may be preferred over high precision if there is little cost associated with a few false alarms (false positives) but high costs associated with missing a positive instance (false negatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
